{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c5e56c39",
   "metadata": {},
   "source": [
    "## üìã Section 1: Setup & Configuration\n",
    "\n",
    "Adjust these parameters to customize your analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f77962a",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Configuration Parameters\n",
    "N_SATELLITES = 150\n",
    "TIME_STEPS = 80\n",
    "NOISE_LEVEL = 0.08\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "LEO_RATIO = 0.6\n",
    "MEO_RATIO = 0.25\n",
    "GEO_RATIO = 0.15\n",
    "\n",
    "HIGH_RISK_THRESHOLD = 5\n",
    "CAUTION_THRESHOLD = 10\n",
    "\n",
    "DBSCAN_EPS = 800\n",
    "DBSCAN_MIN_SAMPLES = 4\n",
    "\n",
    "KMEANS_CLUSTERS = 5\n",
    "\n",
    "TREE_MAX_DEPTH = 10\n",
    "TREE_MIN_SAMPLES_SPLIT = 5\n",
    "\n",
    "KNN_NEIGHBORS = 8\n",
    "KNN_WEIGHTS = 'distance'\n",
    "\n",
    "SVM_C = 5.0\n",
    "SVM_KERNEL = 'rbf'\n",
    "SVM_GAMMA = 'scale'\n",
    "\n",
    "print(\"Parameters configured\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b464ff4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "from sklearn.cluster import DBSCAN, KMeans\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import (accuracy_score, classification_report, \n",
    "                             confusion_matrix, silhouette_score,\n",
    "                             roc_curve, auc, precision_recall_fscore_support)\n",
    "\n",
    "\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"libaries imported successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88303788",
   "metadata": {},
   "source": [
    "## üìä Section 2: Data Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a19e0b48",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(RANDOM_SEED)\n",
    "\n",
    "def generate_satellite_data(n_satellites, timesteps, leo_ratio, meo_ratio, geo_ratio, noise):\n",
    "    orbits = {\n",
    "        'LEO': {'altitude': (400, 2000), 'velocity': (7.5, 8.0)},\n",
    "        'MEO': {'altitude': (2000, 35786), 'velocity': (3.5, 7.5)},\n",
    "        'GEO': {'altitude': (35786, 42164), 'velocity': (3.0, 3.2)}\n",
    "    }\n",
    "    \n",
    "    data = []\n",
    "    n_leo = int(n_satellites * leo_ratio)\n",
    "    n_meo = int(n_satellites * meo_ratio)\n",
    "    n_geo = n_satellites - n_leo - n_meo\n",
    "    \n",
    "    satellite_id = 0\n",
    "    \n",
    "    for orbit_type, count in [('LEO', n_leo), ('MEO', n_meo), ('GEO', n_geo)]:\n",
    "        params = orbits[orbit_type]\n",
    "        \n",
    "        for _ in range(count):\n",
    "            altitude = np.random.uniform(*params['altitude'])\n",
    "            velocity = np.random.uniform(*params['velocity'])\n",
    "            inclination = np.random.uniform(0, 180)\n",
    "            \n",
    "            theta = np.random.uniform(0, 2*np.pi)\n",
    "            phi = np.random.uniform(0, np.pi)\n",
    "            \n",
    "            for t in range(timesteps):\n",
    "                angle = theta + (velocity / altitude) * t * 0.1\n",
    "                \n",
    "                x = altitude * np.sin(phi) * np.cos(angle) + np.random.normal(0, noise * altitude)\n",
    "                y = altitude * np.sin(phi) * np.sin(angle) + np.random.normal(0, noise * altitude)\n",
    "                z = altitude * np.cos(phi) + np.random.normal(0, noise * altitude)\n",
    "                \n",
    "                vx = -velocity * np.sin(angle) + np.random.normal(0, noise)\n",
    "                vy = velocity * np.cos(angle) + np.random.normal(0, noise)\n",
    "                vz = np.random.normal(0, noise * 0.5)\n",
    "                \n",
    "                data.append({\n",
    "                    'satellite_id': f'SAT-{satellite_id:03d}',\n",
    "                    'orbit_type': orbit_type,\n",
    "                    'timestep': t,\n",
    "                    'x': x, 'y': y, 'z': z,\n",
    "                    'vx': vx, 'vy': vy, 'vz': vz,\n",
    "                    'altitude': altitude,\n",
    "                    'velocity': velocity,\n",
    "                    'inclination': inclination\n",
    "                })\n",
    "            \n",
    "            satellite_id += 1\n",
    "    \n",
    "    df = pd.DataFrame(data)\n",
    "    \n",
    "    collision_data = []\n",
    "    for t in range(timesteps):\n",
    "        df_t = df[df['timestep'] == t]\n",
    "        \n",
    "        for i, sat1 in df_t.iterrows():\n",
    "            for j, sat2 in df_t.iterrows():\n",
    "                if sat1['satellite_id'] >= sat2['satellite_id']:\n",
    "                    continue\n",
    "                \n",
    "                distance = np.sqrt((sat1['x'] - sat2['x'])**2 + (sat1['y'] - sat2['y'])**2 + (sat1['z'] - sat2['z'])**2)\n",
    "                rel_vel = np.sqrt((sat1['vx'] - sat2['vx'])**2 + (sat1['vy'] - sat2['vy'])**2 + (sat1['vz'] - sat2['vz'])**2)\n",
    "                \n",
    "                if distance < HIGH_RISK_THRESHOLD:\n",
    "                    risk = 'HIGH_RISK'\n",
    "                elif distance < CAUTION_THRESHOLD:\n",
    "                    risk = 'CAUTION'\n",
    "                else:\n",
    "                    risk = 'SAFE'\n",
    "                \n",
    "                collision_data.append({\n",
    "                    'timestep': t,\n",
    "                    'sat1': sat1['satellite_id'],\n",
    "                    'sat2': sat2['satellite_id'],\n",
    "                    'distance': distance,\n",
    "                    'relative_velocity': rel_vel,\n",
    "                    'sat1_altitude': sat1['altitude'],\n",
    "                    'sat2_altitude': sat2['altitude'],\n",
    "                    'altitude_diff': abs(sat1['altitude'] - sat2['altitude']),\n",
    "                    'sat1_orbit': sat1['orbit_type'],\n",
    "                    'sat2_orbit': sat2['orbit_type'],\n",
    "                    'risk_level': risk\n",
    "                })\n",
    "    \n",
    "    return df, pd.DataFrame(collision_data)\n",
    "\n",
    "satellites_df, collisions_df = generate_satellite_data(N_SATELLITES, TIME_STEPS, LEO_RATIO, MEO_RATIO, GEO_RATIO, NOISE_LEVEL)\n",
    "\n",
    "print(f\"Generated {len(satellites_df)} satellite positions and {len(collisions_df)} collision pairs\")\n",
    "print(f\"\\nRisk distribution:\\n{collisions_df['risk_level'].value_counts()}\")\n",
    "collisions_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1165e21f",
   "metadata": {},
   "source": [
    "## üîç Section 3: Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6106f9e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(18, 12))\n",
    "\n",
    "# 3D satellite positions\n",
    "ax1 = fig.add_subplot(2, 3, 1, projection='3d')\n",
    "for orbit in ['LEO', 'MEO', 'GEO']:\n",
    "    orbit_data = satellites_df[(satellites_df['orbit_type'] == orbit) & (satellites_df['timestep'] == 0)]\n",
    "    ax1.scatter(orbit_data['x'], orbit_data['y'], orbit_data['z'], label=orbit, alpha=0.6, s=50)\n",
    "ax1.set_xlabel('X (km)')\n",
    "ax1.set_ylabel('Y (km)')\n",
    "ax1.set_zlabel('Z (km)')\n",
    "ax1.set_title('Satellite Distribution')\n",
    "ax1.legend()\n",
    "\n",
    "# Distance distribution\n",
    "ax2 = fig.add_subplot(2, 3, 2)\n",
    "ax2.hist(collisions_df['distance'], bins=50, color='steelblue', edgecolor='black', alpha=0.7)\n",
    "ax2.axvline(HIGH_RISK_THRESHOLD, color='red', linestyle='--', label=f'High Risk: {HIGH_RISK_THRESHOLD}km')\n",
    "ax2.axvline(CAUTION_THRESHOLD, color='orange', linestyle='--', label=f'Caution: {CAUTION_THRESHOLD}km')\n",
    "ax2.set_xlabel('Distance (km)')\n",
    "ax2.set_ylabel('Count')\n",
    "ax2.set_title('Distance Distribution')\n",
    "ax2.legend()\n",
    "ax2.grid(alpha=0.3)\n",
    "\n",
    "# Risk levels\n",
    "ax3 = fig.add_subplot(2, 3, 3)\n",
    "risk_counts = collisions_df['risk_level'].value_counts()\n",
    "colors = {'SAFE': 'green', 'CAUTION': 'orange', 'HIGH_RISK': 'red'}\n",
    "ax3.bar(risk_counts.index, risk_counts.values, color=[colors[x] for x in risk_counts.index], alpha=0.7)\n",
    "ax3.set_ylabel('Count')\n",
    "ax3.set_title('Risk Level Distribution')\n",
    "ax3.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Distance vs velocity scatter\n",
    "ax4 = fig.add_subplot(2, 3, 4)\n",
    "scatter = ax4.scatter(collisions_df['distance'], collisions_df['relative_velocity'],\n",
    "                     c=collisions_df['risk_level'].map({'SAFE': 0, 'CAUTION': 1, 'HIGH_RISK': 2}),\n",
    "                     cmap='RdYlGn_r', alpha=0.5, s=10)\n",
    "ax4.set_xlabel('Distance (km)')\n",
    "ax4.set_ylabel('Relative Velocity (km/s)')\n",
    "ax4.set_title('Distance vs Relative Velocity')\n",
    "plt.colorbar(scatter, ax=ax4, label='Risk')\n",
    "ax4.grid(alpha=0.3)\n",
    "\n",
    "# Correlation heatmap\n",
    "ax5 = fig.add_subplot(2, 3, 5)\n",
    "corr_features = ['distance', 'relative_velocity', 'sat1_altitude', 'altitude_diff']\n",
    "corr_matrix = collisions_df[corr_features].corr()\n",
    "sns.heatmap(corr_matrix, annot=True, fmt='.2f', cmap='coolwarm', center=0, ax=ax5)\n",
    "ax5.set_title('Feature Correlation')\n",
    "\n",
    "# Time series\n",
    "ax6 = fig.add_subplot(2, 3, 6)\n",
    "time_stats = collisions_df.groupby('timestep')['distance'].agg(['mean', 'std'])\n",
    "ax6.plot(time_stats.index, time_stats['mean'], color='navy', label='Mean Distance')\n",
    "ax6.fill_between(time_stats.index, time_stats['mean'] - time_stats['std'], time_stats['mean'] + time_stats['std'], alpha=0.3, color='navy')\n",
    "ax6.axhline(HIGH_RISK_THRESHOLD, color='red', linestyle='--', alpha=0.5)\n",
    "ax6.axhline(CAUTION_THRESHOLD, color='orange', linestyle='--', alpha=0.5)\n",
    "ax6.set_xlabel('Timestep')\n",
    "ax6.set_ylabel('Distance (km)')\n",
    "ax6.set_title('Average Distance Over Time')\n",
    "ax6.legend()\n",
    "ax6.grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nStatistical Summary:\")\n",
    "print(collisions_df[['distance', 'relative_velocity', 'altitude_diff']].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afe3378a",
   "metadata": {},
   "source": [
    "## üéØ Section 4: DBSCAN Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46de8b94",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_cluster = collisions_df[['distance', 'relative_velocity', 'sat1_altitude', 'sat2_altitude']].values\n",
    "\n",
    "scaler_dbscan = StandardScaler()\n",
    "X_scaled = scaler_dbscan.fit_transform(X_cluster)\n",
    "\n",
    "dbscan = DBSCAN(eps=DBSCAN_EPS/1000, min_samples=DBSCAN_MIN_SAMPLES)\n",
    "clusters = dbscan.fit_predict(X_scaled)\n",
    "\n",
    "collisions_df['dbscan_cluster'] = clusters\n",
    "\n",
    "n_clusters = len(set(clusters)) - (1 if -1 in clusters else 0)\n",
    "n_noise = list(clusters).count(-1)\n",
    "\n",
    "print(f\"Found {n_clusters} clusters with {n_noise} noise points ({n_noise/len(clusters)*100:.1f}%)\")\n",
    "\n",
    "if n_clusters > 1:\n",
    "    silhouette = silhouette_score(X_scaled[clusters != -1], clusters[clusters != -1])\n",
    "    print(f\"Silhouette Score: {silhouette:.3f}\")\n",
    "\n",
    "fig = plt.figure(figsize=(18, 10))\n",
    "\n",
    "# 3D clusters\n",
    "ax1 = fig.add_subplot(2, 3, 1, projection='3d')\n",
    "scatter = ax1.scatter(collisions_df['distance'], collisions_df['relative_velocity'], collisions_df['altitude_diff'],\n",
    "                     c=clusters, cmap='viridis', alpha=0.6, s=20)\n",
    "ax1.set_xlabel('Distance (km)')\n",
    "ax1.set_ylabel('Relative Velocity (km/s)')\n",
    "ax1.set_zlabel('Altitude Difference (km)')\n",
    "ax1.set_title('DBSCAN Clusters (3D)')\n",
    "plt.colorbar(scatter, ax=ax1, label='Cluster')\n",
    "\n",
    "# 2D projection\n",
    "ax2 = fig.add_subplot(2, 3, 2)\n",
    "scatter2 = ax2.scatter(collisions_df['distance'], collisions_df['relative_velocity'], c=clusters, cmap='viridis', alpha=0.6, s=15)\n",
    "ax2.set_xlabel('Distance (km)')\n",
    "ax2.set_ylabel('Relative Velocity (km/s)')\n",
    "ax2.set_title('Clusters: Distance vs Velocity')\n",
    "plt.colorbar(scatter2, ax=ax2)\n",
    "ax2.grid(alpha=0.3)\n",
    "\n",
    "# Cluster sizes\n",
    "ax3 = fig.add_subplot(2, 3, 3)\n",
    "cluster_counts = pd.Series(clusters).value_counts().sort_index()\n",
    "ax3.bar(cluster_counts.index, cluster_counts.values, color='teal', alpha=0.7, edgecolor='black')\n",
    "ax3.set_xlabel('Cluster ID (-1 = Noise)')\n",
    "ax3.set_ylabel('Points')\n",
    "ax3.set_title('Cluster Sizes')\n",
    "ax3.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Risk per cluster\n",
    "ax4 = fig.add_subplot(2, 3, 4)\n",
    "cluster_risk = pd.crosstab(collisions_df['dbscan_cluster'], collisions_df['risk_level'])\n",
    "cluster_risk.plot(kind='bar', stacked=True, ax=ax4, color=['green', 'orange', 'red'], alpha=0.7)\n",
    "ax4.set_xlabel('Cluster')\n",
    "ax4.set_ylabel('Count')\n",
    "ax4.set_title('Risk Distribution per Cluster')\n",
    "ax4.legend(title='Risk')\n",
    "plt.xticks(rotation=0)\n",
    "ax4.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Eps sensitivity\n",
    "ax5 = fig.add_subplot(2, 3, 5)\n",
    "eps_range = np.linspace(100, 2000, 20)\n",
    "n_clusters_list = []\n",
    "for eps_val in eps_range:\n",
    "    db = DBSCAN(eps=eps_val/1000, min_samples=DBSCAN_MIN_SAMPLES)\n",
    "    labels = db.fit_predict(X_scaled)\n",
    "    n_clusters_list.append(len(set(labels)) - (1 if -1 in labels else 0))\n",
    "\n",
    "ax5.plot(eps_range, n_clusters_list, marker='o', color='navy', linewidth=2)\n",
    "ax5.axvline(DBSCAN_EPS, color='red', linestyle='--', label=f'Current: {DBSCAN_EPS}km')\n",
    "ax5.set_xlabel('Eps (km)')\n",
    "ax5.set_ylabel('Number of Clusters')\n",
    "ax5.set_title('Parameter Sensitivity')\n",
    "ax5.legend()\n",
    "ax5.grid(alpha=0.3)\n",
    "\n",
    "# Cluster statistics\n",
    "ax6 = fig.add_subplot(2, 3, 6)\n",
    "cluster_stats = collisions_df.groupby('dbscan_cluster')[['distance', 'relative_velocity', 'altitude_diff']].mean()\n",
    "sns.heatmap(cluster_stats.T, annot=True, fmt='.1f', cmap='YlOrRd', ax=ax6)\n",
    "ax6.set_xlabel('Cluster')\n",
    "ax6.set_title('Average Features per Cluster')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nCluster Statistics:\")\n",
    "print(cluster_stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64f785d8",
   "metadata": {},
   "source": [
    "## üéØ Section 5: K-Means Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b691ee61",
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters=KMEANS_CLUSTERS, random_state=RANDOM_SEED, n_init=10)\n",
    "kmeans_clusters = kmeans.fit_predict(X_scaled)\n",
    "\n",
    "collisions_df['kmeans_cluster'] = kmeans_clusters\n",
    "\n",
    "inertia = kmeans.inertia_\n",
    "silhouette_km = silhouette_score(X_scaled, kmeans_clusters)\n",
    "\n",
    "print(f\"‚ú® K-Means Clustering Results\")\n",
    "print(f\"Clusters: {KMEANS_CLUSTERS} | Silhouette Score: {silhouette_km:.3f}\")\n",
    "\n",
    "# Single beautiful visualization\n",
    "fig, ax = plt.subplots(figsize=(14, 9), facecolor='white')\n",
    "\n",
    "# Create color map for risk levels\n",
    "risk_colors = {'SAFE': '#2ecc71', 'CAUTION': '#f39c12', 'HIGH_RISK': '#e74c3c'}\n",
    "colors = [risk_colors[risk] for risk in collisions_df['risk_level']]\n",
    "\n",
    "# Main scatter plot\n",
    "scatter = ax.scatter(collisions_df['distance'], \n",
    "                    collisions_df['relative_velocity'],\n",
    "                    c=colors,\n",
    "                    s=80,\n",
    "                    alpha=0.6,\n",
    "                    edgecolors='white',\n",
    "                    linewidth=0.5)\n",
    "\n",
    "# Add cluster centroids\n",
    "centroids = scaler_dbscan.inverse_transform(kmeans.cluster_centers_)\n",
    "ax.scatter(centroids[:, 0], centroids[:, 1], \n",
    "          c='navy', \n",
    "          marker='‚òÖ', \n",
    "          s=800, \n",
    "          edgecolors='gold',\n",
    "          linewidths=3,\n",
    "          label='Cluster Centers',\n",
    "          zorder=5)\n",
    "\n",
    "# Annotate each centroid\n",
    "for i, (x, y) in enumerate(centroids[:, :2]):\n",
    "    ax.annotate(f'C{i}', \n",
    "               xy=(x, y), \n",
    "               xytext=(0, 0), \n",
    "               textcoords='offset points',\n",
    "               ha='center', \n",
    "               va='center',\n",
    "               fontsize=11,\n",
    "               fontweight='bold',\n",
    "               color='white',\n",
    "               zorder=6)\n",
    "\n",
    "# Styling\n",
    "ax.set_xlabel('Distance (km)', fontsize=13, fontweight='bold')\n",
    "ax.set_ylabel('Relative Velocity (km/s)', fontsize=13, fontweight='bold')\n",
    "ax.set_title('üõ∞Ô∏è Satellite Collision Risk Clustering\\nK-Means Analysis', \n",
    "            fontsize=16, fontweight='bold', pad=20)\n",
    "\n",
    "# Create custom legend\n",
    "from matplotlib.patches import Patch\n",
    "legend_elements = [\n",
    "    Patch(facecolor='#2ecc71', label='SAFE', alpha=0.7),\n",
    "    Patch(facecolor='#f39c12', label='CAUTION', alpha=0.7),\n",
    "    Patch(facecolor='#e74c3c', label='HIGH RISK', alpha=0.7),\n",
    "    plt.Line2D([0], [0], marker='‚òÖ', color='w', markerfacecolor='navy', \n",
    "              markeredgecolor='gold', markersize=15, label='Centroids', markeredgewidth=2)\n",
    "]\n",
    "ax.legend(handles=legend_elements, loc='upper right', fontsize=11, framealpha=0.95)\n",
    "\n",
    "# Add grid\n",
    "ax.grid(True, alpha=0.3, linestyle='--', linewidth=0.5)\n",
    "\n",
    "# Add statistics box\n",
    "stats_text = f'Clusters: {KMEANS_CLUSTERS}\\nSilhouette: {silhouette_km:.3f}\\nPoints: {len(collisions_df):,}'\n",
    "ax.text(0.02, 0.98, stats_text, \n",
    "       transform=ax.transAxes,\n",
    "       fontsize=10,\n",
    "       verticalalignment='top',\n",
    "       bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.8))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nüìä Cluster Distribution:\")\n",
    "for i in range(KMEANS_CLUSTERS):\n",
    "    cluster_data = collisions_df[collisions_df['kmeans_cluster'] == i]\n",
    "    risk_counts = cluster_data['risk_level'].value_counts()\n",
    "    print(f\"  Cluster {i}: {len(cluster_data)} points - {dict(risk_counts)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cdbc827",
   "metadata": {},
   "source": [
    "## üå≥ Section 6: Decision Tree Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94814d29",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_cols = ['distance', 'relative_velocity', 'sat1_altitude', 'sat2_altitude', 'altitude_diff']\n",
    "X = collisions_df[feature_cols].values\n",
    "y = collisions_df['risk_level'].values\n",
    "\n",
    "label_map = {'SAFE': 0, 'CAUTION': 1, 'HIGH_RISK': 2}\n",
    "y_encoded = np.array([label_map[label] for label in y])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.2, random_state=RANDOM_SEED, stratify=y_encoded)\n",
    "\n",
    "dt_classifier = DecisionTreeClassifier(max_depth=TREE_MAX_DEPTH, min_samples_split=TREE_MIN_SAMPLES_SPLIT, random_state=RANDOM_SEED)\n",
    "dt_classifier.fit(X_train, y_train)\n",
    "\n",
    "y_pred_dt = dt_classifier.predict(X_test)\n",
    "accuracy_dt = accuracy_score(y_test, y_pred_dt)\n",
    "\n",
    "print(f\"Accuracy: {accuracy_dt:.3f}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred_dt, target_names=['SAFE', 'CAUTION', 'HIGH_RISK']))\n",
    "\n",
    "fig = plt.figure(figsize=(18, 12))\n",
    "\n",
    "# Tree structure\n",
    "ax1 = fig.add_subplot(2, 3, 1)\n",
    "plot_tree(dt_classifier, feature_names=feature_cols, class_names=['SAFE', 'CAUTION', 'HIGH_RISK'],\n",
    "         filled=True, max_depth=3, fontsize=8, ax=ax1)\n",
    "ax1.set_title('Decision Tree (depth=3)')\n",
    "\n",
    "# Feature importance\n",
    "ax2 = fig.add_subplot(2, 3, 2)\n",
    "importances = dt_classifier.feature_importances_\n",
    "indices = np.argsort(importances)[::-1]\n",
    "ax2.barh(range(len(importances)), importances[indices], color='steelblue', alpha=0.7)\n",
    "ax2.set_yticks(range(len(importances)))\n",
    "ax2.set_yticklabels([feature_cols[i] for i in indices])\n",
    "ax2.set_xlabel('Importance')\n",
    "ax2.set_title('Feature Importance')\n",
    "ax2.grid(axis='x', alpha=0.3)\n",
    "\n",
    "# Confusion matrix\n",
    "ax3 = fig.add_subplot(2, 3, 3)\n",
    "cm_dt = confusion_matrix(y_test, y_pred_dt)\n",
    "sns.heatmap(cm_dt, annot=True, fmt='d', cmap='Blues', ax=ax3,\n",
    "           xticklabels=['SAFE', 'CAUTION', 'HIGH_RISK'], yticklabels=['SAFE', 'CAUTION', 'HIGH_RISK'])\n",
    "ax3.set_xlabel('Predicted')\n",
    "ax3.set_ylabel('Actual')\n",
    "ax3.set_title('Confusion Matrix')\n",
    "\n",
    "# Depth vs accuracy\n",
    "ax4 = fig.add_subplot(2, 3, 4)\n",
    "depths = range(1, 21)\n",
    "train_scores = []\n",
    "test_scores = []\n",
    "for depth in depths:\n",
    "    dt_temp = DecisionTreeClassifier(max_depth=depth, random_state=RANDOM_SEED)\n",
    "    dt_temp.fit(X_train, y_train)\n",
    "    train_scores.append(dt_temp.score(X_train, y_train))\n",
    "    test_scores.append(dt_temp.score(X_test, y_test))\n",
    "\n",
    "ax4.plot(depths, train_scores, 'o-', label='Train', linewidth=2)\n",
    "ax4.plot(depths, test_scores, 's-', label='Test', linewidth=2)\n",
    "ax4.axvline(TREE_MAX_DEPTH, color='red', linestyle='--', label=f'Current: {TREE_MAX_DEPTH}')\n",
    "ax4.set_xlabel('Max Depth')\n",
    "ax4.set_ylabel('Accuracy')\n",
    "ax4.set_title('Complexity vs Accuracy')\n",
    "ax4.legend()\n",
    "ax4.grid(alpha=0.3)\n",
    "\n",
    "# Metrics by class\n",
    "ax5 = fig.add_subplot(2, 3, 5)\n",
    "precision, recall, f1, _ = precision_recall_fscore_support(y_test, y_pred_dt)\n",
    "x_pos = np.arange(3)\n",
    "width = 0.25\n",
    "ax5.bar(x_pos - width, precision, width, label='Precision', alpha=0.8)\n",
    "ax5.bar(x_pos, recall, width, label='Recall', alpha=0.8)\n",
    "ax5.bar(x_pos + width, f1, width, label='F1', alpha=0.8)\n",
    "ax5.set_xticks(x_pos)\n",
    "ax5.set_xticklabels(['SAFE', 'CAUTION', 'HIGH_RISK'])\n",
    "ax5.set_ylabel('Score')\n",
    "ax5.set_title('Metrics by Risk Level')\n",
    "ax5.legend()\n",
    "ax5.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Prediction distribution\n",
    "ax6 = fig.add_subplot(2, 3, 6)\n",
    "pred_df = pd.DataFrame({'Actual': y_test, 'Predicted': y_pred_dt})\n",
    "pred_counts = pd.crosstab(pred_df['Actual'], pred_df['Predicted'])\n",
    "sns.heatmap(pred_counts, annot=True, fmt='d', cmap='YlGnBu', ax=ax6,\n",
    "           xticklabels=['SAFE', 'CAUTION', 'HIGH_RISK'], yticklabels=['SAFE', 'CAUTION', 'HIGH_RISK'])\n",
    "ax6.set_xlabel('Predicted')\n",
    "ax6.set_ylabel('Actual')\n",
    "ax6.set_title('Prediction Distribution')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0929f360",
   "metadata": {},
   "source": [
    "## üî¢ Section 7: KNN Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5847c02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# üî¢ K-NEAREST NEIGHBORS CLASSIFIER\n",
    "# ============================================\n",
    "\n",
    "# Standardize features for KNN (distance-based algorithm)\n",
    "scaler_knn = StandardScaler()\n",
    "X_train_scaled = scaler_knn.fit_transform(X_train)\n",
    "X_test_scaled = scaler_knn.transform(X_test)\n",
    "\n",
    "# Train KNN\n",
    "print(f\"üîÑ Training KNN (n_neighbors={KNN_NEIGHBORS}, weights='{KNN_WEIGHTS}')...\")\n",
    "knn_classifier = KNeighborsClassifier(\n",
    "    n_neighbors=KNN_NEIGHBORS,\n",
    "    weights=KNN_WEIGHTS,\n",
    "    metric='euclidean'\n",
    ")\n",
    "knn_classifier.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred_knn = knn_classifier.predict(X_test_scaled)\n",
    "accuracy_knn = accuracy_score(y_test, y_pred_knn)\n",
    "\n",
    "print(f\"‚úÖ Training complete!\")\n",
    "print(f\"üìä Accuracy: {accuracy_knn:.3f}\")\n",
    "print(f\"\\nüìã Classification Report:\")\n",
    "print(classification_report(y_test, y_pred_knn, \n",
    "                          target_names=['SAFE', 'CAUTION', 'HIGH_RISK']))\n",
    "\n",
    "# Visualizations\n",
    "fig = plt.figure(figsize=(18, 12))\n",
    "\n",
    "# 1. K vs Accuracy (finding optimal K)\n",
    "ax1 = fig.add_subplot(2, 3, 1)\n",
    "k_values = range(1, 31, 2)\n",
    "train_accuracies = []\n",
    "test_accuracies = []\n",
    "\n",
    "for k in k_values:\n",
    "    knn_temp = KNeighborsClassifier(n_neighbors=k, weights=KNN_WEIGHTS)\n",
    "    knn_temp.fit(X_train_scaled, y_train)\n",
    "    train_accuracies.append(knn_temp.score(X_train_scaled, y_train))\n",
    "    test_accuracies.append(knn_temp.score(X_test_scaled, y_test))\n",
    "\n",
    "ax1.plot(k_values, train_accuracies, 'o-', label='Train Accuracy', linewidth=2)\n",
    "ax1.plot(k_values, test_accuracies, 's-', label='Test Accuracy', linewidth=2)\n",
    "ax1.axvline(KNN_NEIGHBORS, color='red', linestyle='--', \n",
    "           label=f'Current K={KNN_NEIGHBORS}')\n",
    "ax1.set_xlabel('Number of Neighbors (K)')\n",
    "ax1.set_ylabel('Accuracy')\n",
    "ax1.set_title('Optimal K Selection', fontsize=12, fontweight='bold')\n",
    "ax1.legend()\n",
    "ax1.grid(alpha=0.3)\n",
    "\n",
    "# 2. Confusion Matrix\n",
    "ax2 = fig.add_subplot(2, 3, 2)\n",
    "cm_knn = confusion_matrix(y_test, y_pred_knn)\n",
    "sns.heatmap(cm_knn, annot=True, fmt='d', cmap='Greens', ax=ax2,\n",
    "           xticklabels=['SAFE', 'CAUTION', 'HIGH_RISK'],\n",
    "           yticklabels=['SAFE', 'CAUTION', 'HIGH_RISK'])\n",
    "ax2.set_xlabel('Predicted')\n",
    "ax2.set_ylabel('Actual')\n",
    "ax2.set_title('Confusion Matrix', fontsize=12, fontweight='bold')\n",
    "\n",
    "# 3. Decision Boundary (2D projection: distance vs relative_velocity)\n",
    "ax3 = fig.add_subplot(2, 3, 3)\n",
    "# Create mesh grid\n",
    "x_min, x_max = X_test_scaled[:, 0].min() - 1, X_test_scaled[:, 0].max() + 1\n",
    "y_min, y_max = X_test_scaled[:, 1].min() - 1, X_test_scaled[:, 1].max() + 1\n",
    "xx, yy = np.meshgrid(np.linspace(x_min, x_max, 100),\n",
    "                     np.linspace(y_min, y_max, 100))\n",
    "\n",
    "# Predict on mesh (using mean values for other features)\n",
    "mesh_features = np.c_[xx.ravel(), yy.ravel(), \n",
    "                      np.full(xx.ravel().shape, X_test_scaled[:, 2].mean()),\n",
    "                      np.full(xx.ravel().shape, X_test_scaled[:, 3].mean()),\n",
    "                      np.full(xx.ravel().shape, X_test_scaled[:, 4].mean())]\n",
    "Z = knn_classifier.predict(mesh_features)\n",
    "Z = Z.reshape(xx.shape)\n",
    "\n",
    "# Plot decision boundary\n",
    "ax3.contourf(xx, yy, Z, alpha=0.3, cmap='RdYlGn', levels=2)\n",
    "scatter = ax3.scatter(X_test_scaled[:, 0], X_test_scaled[:, 1], \n",
    "                     c=y_test, cmap='RdYlGn', edgecolors='black', s=30, alpha=0.7)\n",
    "ax3.set_xlabel('Distance (scaled)')\n",
    "ax3.set_ylabel('Relative Velocity (scaled)')\n",
    "ax3.set_title('KNN Decision Boundary (2D)', fontsize=12, fontweight='bold')\n",
    "plt.colorbar(scatter, ax=ax3, ticks=[0, 1, 2], \n",
    "            label='Risk Level')\n",
    "\n",
    "# 4. Weights Comparison (uniform vs distance)\n",
    "ax4 = fig.add_subplot(2, 3, 4)\n",
    "weight_types = ['uniform', 'distance']\n",
    "accuracies_by_weight = []\n",
    "\n",
    "for weight in weight_types:\n",
    "    knn_w = KNeighborsClassifier(n_neighbors=KNN_NEIGHBORS, weights=weight)\n",
    "    knn_w.fit(X_train_scaled, y_train)\n",
    "    accuracies_by_weight.append(knn_w.score(X_test_scaled, y_test))\n",
    "\n",
    "bars = ax4.bar(weight_types, accuracies_by_weight, color=['coral', 'teal'], alpha=0.7)\n",
    "ax4.set_ylabel('Accuracy')\n",
    "ax4.set_title('Weight Type Comparison', fontsize=12, fontweight='bold')\n",
    "ax4.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Highlight current weight\n",
    "if KNN_WEIGHTS == 'uniform':\n",
    "    bars[0].set_edgecolor('red')\n",
    "    bars[0].set_linewidth(3)\n",
    "else:\n",
    "    bars[1].set_edgecolor('red')\n",
    "    bars[1].set_linewidth(3)\n",
    "\n",
    "# 5. Precision, Recall, F1 by Class\n",
    "ax5 = fig.add_subplot(2, 3, 5)\n",
    "precision_knn, recall_knn, f1_knn, _ = precision_recall_fscore_support(y_test, y_pred_knn)\n",
    "x_pos = np.arange(3)\n",
    "width = 0.25\n",
    "ax5.bar(x_pos - width, precision_knn, width, label='Precision', alpha=0.8, color='navy')\n",
    "ax5.bar(x_pos, recall_knn, width, label='Recall', alpha=0.8, color='darkgreen')\n",
    "ax5.bar(x_pos + width, f1_knn, width, label='F1-Score', alpha=0.8, color='darkred')\n",
    "ax5.set_xticks(x_pos)\n",
    "ax5.set_xticklabels(['SAFE', 'CAUTION', 'HIGH_RISK'])\n",
    "ax5.set_ylabel('Score')\n",
    "ax5.set_ylim([0, 1.1])\n",
    "ax5.set_title('Metrics by Risk Level', fontsize=12, fontweight='bold')\n",
    "ax5.legend()\n",
    "ax5.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# 6. Sample Neighbor Visualization\n",
    "ax6 = fig.add_subplot(2, 3, 6)\n",
    "# Pick a random test sample\n",
    "sample_idx = np.random.randint(0, len(X_test_scaled))\n",
    "sample = X_test_scaled[sample_idx:sample_idx+1]\n",
    "\n",
    "# Find K nearest neighbors\n",
    "distances, indices = knn_classifier.kneighbors(sample, n_neighbors=KNN_NEIGHBORS)\n",
    "\n",
    "# Plot the sample and its neighbors (using first 2 features)\n",
    "ax6.scatter(X_train_scaled[:, 0], X_train_scaled[:, 1], \n",
    "           c='lightgray', alpha=0.3, s=20, label='Other Training Data')\n",
    "ax6.scatter(X_train_scaled[indices[0], 0], X_train_scaled[indices[0], 1],\n",
    "           c=y_train[indices[0]], cmap='RdYlGn', s=100, \n",
    "           edgecolors='black', linewidths=2, label='K Neighbors')\n",
    "ax6.scatter(sample[0, 0], sample[0, 1], c='blue', marker='*', \n",
    "           s=500, edgecolors='black', linewidths=2, label='Test Sample')\n",
    "ax6.set_xlabel('Distance (scaled)')\n",
    "ax6.set_ylabel('Relative Velocity (scaled)')\n",
    "ax6.set_title(f'Sample Prediction: {KNN_NEIGHBORS} Nearest Neighbors', \n",
    "             fontsize=12, fontweight='bold')\n",
    "ax6.legend()\n",
    "ax6.grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nüéØ Sample Test Case:\")\n",
    "print(f\"   Predicted: {['SAFE', 'CAUTION', 'HIGH_RISK'][y_pred_knn[sample_idx]]}\")\n",
    "print(f\"   Actual: {['SAFE', 'CAUTION', 'HIGH_RISK'][y_test[sample_idx]]}\")\n",
    "print(f\"   Neighbor distances: {distances[0][:5]}\")  # Show first 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39232fec",
   "metadata": {},
   "source": [
    "# ============================================\n",
    "# ‚ö° SUPPORT VECTOR MACHINE CLASSIFIER\n",
    "# ============================================\n",
    "\n",
    "# Train SVM\n",
    "print(f\"üîÑ Training SVM (kernel='{SVM_KERNEL}', C={SVM_C}, gamma='{SVM_GAMMA}')...\")\n",
    "svm_classifier = SVC(\n",
    "    C=SVM_C,\n",
    "    kernel=SVM_KERNEL,\n",
    "    gamma=SVM_GAMMA,\n",
    "    random_state=RANDOM_SEED,\n",
    "    probability=True  # Enable probability estimates for ROC curve\n",
    ")\n",
    "svm_classifier.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred_svm = svm_classifier.predict(X_test_scaled)\n",
    "accuracy_svm = accuracy_score(y_test, y_pred_svm)\n",
    "\n",
    "print(f\"‚úÖ Training complete!\")\n",
    "print(f\"üìä Accuracy: {accuracy_svm:.3f}\")\n",
    "print(f\"üìä Number of support vectors: {svm_classifier.n_support_}\")\n",
    "print(f\"\\nüìã Classification Report:\")\n",
    "print(classification_report(y_test, y_pred_svm, \n",
    "                          target_names=['SAFE', 'CAUTION', 'HIGH_RISK']))\n",
    "\n",
    "# Visualizations\n",
    "fig = plt.figure(figsize=(18, 12))\n",
    "\n",
    "# 1. Kernel Comparison\n",
    "ax1 = fig.add_subplot(2, 3, 1)\n",
    "kernels = ['linear', 'rbf', 'poly']\n",
    "kernel_accuracies = []\n",
    "\n",
    "for kernel in kernels:\n",
    "    svm_temp = SVC(kernel=kernel, C=SVM_C, gamma=SVM_GAMMA, random_state=RANDOM_SEED)\n",
    "    svm_temp.fit(X_train_scaled, y_train)\n",
    "    kernel_accuracies.append(svm_temp.score(X_test_scaled, y_test))\n",
    "\n",
    "bars = ax1.bar(kernels, kernel_accuracies, color=['coral', 'teal', 'purple'], alpha=0.7)\n",
    "ax1.set_ylabel('Accuracy')\n",
    "ax1.set_title('Kernel Comparison', fontsize=12, fontweight='bold')\n",
    "ax1.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Highlight current kernel\n",
    "if SVM_KERNEL in kernels:\n",
    "    idx = kernels.index(SVM_KERNEL)\n",
    "    bars[idx].set_edgecolor('red')\n",
    "    bars[idx].set_linewidth(3)\n",
    "\n",
    "# 2. Confusion Matrix\n",
    "ax2 = fig.add_subplot(2, 3, 2)\n",
    "cm_svm = confusion_matrix(y_test, y_pred_svm)\n",
    "sns.heatmap(cm_svm, annot=True, fmt='d', cmap='Purples', ax=ax2,\n",
    "           xticklabels=['SAFE', 'CAUTION', 'HIGH_RISK'],\n",
    "           yticklabels=['SAFE', 'CAUTION', 'HIGH_RISK'])\n",
    "ax2.set_xlabel('Predicted')\n",
    "ax2.set_ylabel('Actual')\n",
    "ax2.set_title('Confusion Matrix', fontsize=12, fontweight='bold')\n",
    "\n",
    "# 3. Decision Boundary (2D projection)\n",
    "ax3 = fig.add_subplot(2, 3, 3)\n",
    "x_min, x_max = X_test_scaled[:, 0].min() - 1, X_test_scaled[:, 0].max() + 1\n",
    "y_min, y_max = X_test_scaled[:, 1].min() - 1, X_test_scaled[:, 1].max() + 1\n",
    "xx, yy = np.meshgrid(np.linspace(x_min, x_max, 100),\n",
    "                     np.linspace(y_min, y_max, 100))\n",
    "\n",
    "mesh_features = np.c_[xx.ravel(), yy.ravel(), \n",
    "                      np.full(xx.ravel().shape, X_test_scaled[:, 2].mean()),\n",
    "                      np.full(xx.ravel().shape, X_test_scaled[:, 3].mean()),\n",
    "                      np.full(xx.ravel().shape, X_test_scaled[:, 4].mean())]\n",
    "Z = svm_classifier.predict(mesh_features)\n",
    "Z = Z.reshape(xx.shape)\n",
    "\n",
    "ax3.contourf(xx, yy, Z, alpha=0.3, cmap='RdYlGn', levels=2)\n",
    "scatter = ax3.scatter(X_test_scaled[:, 0], X_test_scaled[:, 1], \n",
    "                     c=y_test, cmap='RdYlGn', edgecolors='black', s=30, alpha=0.7)\n",
    "ax3.set_xlabel('Distance (scaled)')\n",
    "ax3.set_ylabel('Relative Velocity (scaled)')\n",
    "ax3.set_title('SVM Decision Boundary', fontsize=12, fontweight='bold')\n",
    "plt.colorbar(scatter, ax=ax3, ticks=[0, 1, 2], label='Risk Level')\n",
    "\n",
    "# 4. C Parameter Sensitivity\n",
    "ax4 = fig.add_subplot(2, 3, 4)\n",
    "c_values = [0.1, 0.5, 1, 5, 10, 50, 100]\n",
    "c_train_scores = []\n",
    "c_test_scores = []\n",
    "\n",
    "for c in c_values:\n",
    "    svm_c = SVC(C=c, kernel=SVM_KERNEL, gamma=SVM_GAMMA, random_state=RANDOM_SEED)\n",
    "    svm_c.fit(X_train_scaled, y_train)\n",
    "    c_train_scores.append(svm_c.score(X_train_scaled, y_train))\n",
    "    c_test_scores.append(svm_c.score(X_test_scaled, y_test))\n",
    "\n",
    "ax4.semilogx(c_values, c_train_scores, 'o-', label='Train Accuracy', linewidth=2)\n",
    "ax4.semilogx(c_values, c_test_scores, 's-', label='Test Accuracy', linewidth=2)\n",
    "ax4.axvline(SVM_C, color='red', linestyle='--', label=f'Current C={SVM_C}')\n",
    "ax4.set_xlabel('C (Regularization)')\n",
    "ax4.set_ylabel('Accuracy')\n",
    "ax4.set_title('C Parameter Sensitivity', fontsize=12, fontweight='bold')\n",
    "ax4.legend()\n",
    "ax4.grid(alpha=0.3)\n",
    "\n",
    "# 5. ROC Curves (One-vs-Rest)\n",
    "ax5 = fig.add_subplot(2, 3, 5)\n",
    "y_score = svm_classifier.predict_proba(X_test_scaled)\n",
    "\n",
    "for i, class_name in enumerate(['SAFE', 'CAUTION', 'HIGH_RISK']):\n",
    "    # Binarize the labels\n",
    "    y_test_binary = (y_test == i).astype(int)\n",
    "    fpr, tpr, _ = roc_curve(y_test_binary, y_score[:, i])\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    ax5.plot(fpr, tpr, linewidth=2, \n",
    "            label=f'{class_name} (AUC = {roc_auc:.2f})')\n",
    "\n",
    "ax5.plot([0, 1], [0, 1], 'k--', linewidth=1, label='Random')\n",
    "ax5.set_xlabel('False Positive Rate')\n",
    "ax5.set_ylabel('True Positive Rate')\n",
    "ax5.set_title('ROC Curves (One-vs-Rest)', fontsize=12, fontweight='bold')\n",
    "ax5.legend()\n",
    "ax5.grid(alpha=0.3)\n",
    "\n",
    "# 6. Precision, Recall, F1 by Class\n",
    "ax6 = fig.add_subplot(2, 3, 6)\n",
    "precision_svm, recall_svm, f1_svm, _ = precision_recall_fscore_support(y_test, y_pred_svm)\n",
    "x_pos = np.arange(3)\n",
    "width = 0.25\n",
    "ax6.bar(x_pos - width, precision_svm, width, label='Precision', alpha=0.8, color='darkblue')\n",
    "ax6.bar(x_pos, recall_svm, width, label='Recall', alpha=0.8, color='darkgreen')\n",
    "ax6.bar(x_pos + width, f1_svm, width, label='F1-Score', alpha=0.8, color='darkred')\n",
    "ax6.set_xticks(x_pos)\n",
    "ax6.set_xticklabels(['SAFE', 'CAUTION', 'HIGH_RISK'])\n",
    "ax6.set_ylabel('Score')\n",
    "ax6.set_ylim([0, 1.1])\n",
    "ax6.set_title('Metrics by Risk Level', fontsize=12, fontweight='bold')\n",
    "ax6.legend()\n",
    "ax6.grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\n‚ö° SVM Support Vectors per Class:\")\n",
    "for i, class_name in enumerate(['SAFE', 'CAUTION', 'HIGH_RISK']):\n",
    "    print(f\"   {class_name}: {svm_classifier.n_support_[i]} support vectors\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6782cdb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a638252a",
   "metadata": {},
   "source": [
    "# ============================================\n",
    "# üìà ALGORITHM COMPARISON DASHBOARD\n",
    "# ============================================\n",
    "\n",
    "# Collect all results\n",
    "algorithms = ['Decision Tree', 'KNN', 'SVM']\n",
    "predictions = [y_pred_dt, y_pred_knn, y_pred_svm]\n",
    "accuracies = [accuracy_dt, accuracy_knn, accuracy_svm]\n",
    "\n",
    "# Calculate comprehensive metrics\n",
    "all_metrics = []\n",
    "for alg, y_pred in zip(algorithms, predictions):\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(y_test, y_pred, average='weighted')\n",
    "    all_metrics.append({\n",
    "        'Algorithm': alg,\n",
    "        'Accuracy': accuracy_score(y_test, y_pred),\n",
    "        'Precision': precision,\n",
    "        'Recall': recall,\n",
    "        'F1-Score': f1\n",
    "    })\n",
    "\n",
    "metrics_df = pd.DataFrame(all_metrics)\n",
    "\n",
    "print(\"üìä ALGORITHM COMPARISON\")\n",
    "print(\"=\" * 70)\n",
    "print(metrics_df.to_string(index=False))\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Visualizations\n",
    "fig = plt.figure(figsize=(18, 12))\n",
    "\n",
    "# 1. Accuracy Comparison\n",
    "ax1 = fig.add_subplot(2, 3, 1)\n",
    "bars = ax1.bar(algorithms, accuracies, color=['forestgreen', 'steelblue', 'purple'], alpha=0.7)\n",
    "ax1.set_ylabel('Accuracy')\n",
    "ax1.set_ylim([0, 1.1])\n",
    "ax1.set_title('Accuracy Comparison', fontsize=12, fontweight='bold')\n",
    "ax1.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar in bars:\n",
    "    height = bar.get_height()\n",
    "    ax1.text(bar.get_x() + bar.get_width()/2., height,\n",
    "            f'{height:.3f}', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "# 2. Metrics Radar Chart\n",
    "ax2 = fig.add_subplot(2, 3, 2, projection='polar')\n",
    "categories = ['Accuracy', 'Precision', 'Recall', 'F1-Score']\n",
    "N = len(categories)\n",
    "angles = [n / float(N) * 2 * np.pi for n in range(N)]\n",
    "angles += angles[:1]\n",
    "\n",
    "for i, alg in enumerate(algorithms):\n",
    "    values = metrics_df.iloc[i][1:].values.tolist()\n",
    "    values += values[:1]\n",
    "    ax2.plot(angles, values, 'o-', linewidth=2, label=alg)\n",
    "    ax2.fill(angles, values, alpha=0.15)\n",
    "\n",
    "ax2.set_xticks(angles[:-1])\n",
    "ax2.set_xticklabels(categories)\n",
    "ax2.set_ylim(0, 1)\n",
    "ax2.set_title('Performance Radar Chart', fontsize=12, fontweight='bold', pad=20)\n",
    "ax2.legend(loc='upper right', bbox_to_anchor=(1.3, 1.1))\n",
    "ax2.grid(True)\n",
    "\n",
    "# 3. Confusion Matrices Comparison\n",
    "ax3 = fig.add_subplot(2, 3, 3)\n",
    "confusion_matrices = [\n",
    "    confusion_matrix(y_test, y_pred_dt),\n",
    "    confusion_matrix(y_test, y_pred_knn),\n",
    "    confusion_matrix(y_test, y_pred_svm)\n",
    "]\n",
    "\n",
    "# Combine confusion matrices side by side\n",
    "combined_cm = np.hstack(confusion_matrices)\n",
    "sns.heatmap(combined_cm, annot=True, fmt='d', cmap='YlOrRd', ax=ax3, cbar=False)\n",
    "ax3.set_ylabel('Actual')\n",
    "ax3.set_title('Confusion Matrices: DT | KNN | SVM', fontsize=12, fontweight='bold')\n",
    "ax3.set_xticks([1.5, 4.5, 7.5])\n",
    "ax3.set_xticklabels(['DT', 'KNN', 'SVM'])\n",
    "ax3.set_yticks([0.5, 1.5, 2.5])\n",
    "ax3.set_yticklabels(['SAFE', 'CAUTION', 'HIGH_RISK'])\n",
    "\n",
    "# 4. Per-Class F1 Scores\n",
    "ax4 = fig.add_subplot(2, 3, 4)\n",
    "f1_scores = {\n",
    "    'Decision Tree': precision_recall_fscore_support(y_test, y_pred_dt)[2],\n",
    "    'KNN': precision_recall_fscore_support(y_test, y_pred_knn)[2],\n",
    "    'SVM': precision_recall_fscore_support(y_test, y_pred_svm)[2]\n",
    "}\n",
    "\n",
    "x = np.arange(3)\n",
    "width = 0.25\n",
    "for i, (alg, scores) in enumerate(f1_scores.items()):\n",
    "    ax4.bar(x + i*width, scores, width, label=alg, alpha=0.8)\n",
    "\n",
    "ax4.set_xticks(x + width)\n",
    "ax4.set_xticklabels(['SAFE', 'CAUTION', 'HIGH_RISK'])\n",
    "ax4.set_ylabel('F1-Score')\n",
    "ax4.set_ylim([0, 1.1])\n",
    "ax4.set_title('F1-Score by Risk Level', fontsize=12, fontweight='bold')\n",
    "ax4.legend()\n",
    "ax4.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# 5. ROC Curves Comparison (for one class - HIGH_RISK)\n",
    "ax5 = fig.add_subplot(2, 3, 5)\n",
    "y_test_binary = (y_test == 2).astype(int)  # HIGH_RISK class\n",
    "\n",
    "# Decision Tree\n",
    "dt_proba = DecisionTreeClassifier(max_depth=TREE_MAX_DEPTH, random_state=RANDOM_SEED)\n",
    "dt_proba.fit(X_train, y_train)\n",
    "dt_scores = dt_proba.predict_proba(X_test)[:, 2]\n",
    "fpr_dt, tpr_dt, _ = roc_curve(y_test_binary, dt_scores)\n",
    "auc_dt = auc(fpr_dt, tpr_dt)\n",
    "\n",
    "# KNN\n",
    "knn_scores = knn_classifier.predict_proba(X_test_scaled)[:, 2]\n",
    "fpr_knn, tpr_knn, _ = roc_curve(y_test_binary, knn_scores)\n",
    "auc_knn = auc(fpr_knn, tpr_knn)\n",
    "\n",
    "# SVM\n",
    "svm_scores = svm_classifier.predict_proba(X_test_scaled)[:, 2]\n",
    "fpr_svm, tpr_svm, _ = roc_curve(y_test_binary, svm_scores)\n",
    "auc_svm = auc(fpr_svm, tpr_svm)\n",
    "\n",
    "ax5.plot(fpr_dt, tpr_dt, linewidth=2, label=f'Decision Tree (AUC={auc_dt:.2f})')\n",
    "ax5.plot(fpr_knn, tpr_knn, linewidth=2, label=f'KNN (AUC={auc_knn:.2f})')\n",
    "ax5.plot(fpr_svm, tpr_svm, linewidth=2, label=f'SVM (AUC={auc_svm:.2f})')\n",
    "ax5.plot([0, 1], [0, 1], 'k--', linewidth=1, label='Random')\n",
    "ax5.set_xlabel('False Positive Rate')\n",
    "ax5.set_ylabel('True Positive Rate')\n",
    "ax5.set_title('ROC Curves (HIGH_RISK Detection)', fontsize=12, fontweight='bold')\n",
    "ax5.legend()\n",
    "ax5.grid(alpha=0.3)\n",
    "\n",
    "# 6. Prediction Agreement Heatmap\n",
    "ax6 = fig.add_subplot(2, 3, 6)\n",
    "agreement_matrix = np.zeros((3, 3))\n",
    "agreement_matrix[0, 1] = np.sum(y_pred_dt == y_pred_knn) / len(y_pred_dt)\n",
    "agreement_matrix[0, 2] = np.sum(y_pred_dt == y_pred_svm) / len(y_pred_dt)\n",
    "agreement_matrix[1, 2] = np.sum(y_pred_knn == y_pred_svm) / len(y_pred_knn)\n",
    "agreement_matrix[1, 0] = agreement_matrix[0, 1]\n",
    "agreement_matrix[2, 0] = agreement_matrix[0, 2]\n",
    "agreement_matrix[2, 1] = agreement_matrix[1, 2]\n",
    "np.fill_diagonal(agreement_matrix, 1.0)\n",
    "\n",
    "sns.heatmap(agreement_matrix, annot=True, fmt='.2f', cmap='coolwarm', \n",
    "           xticklabels=['DT', 'KNN', 'SVM'],\n",
    "           yticklabels=['DT', 'KNN', 'SVM'],\n",
    "           ax=ax6, vmin=0, vmax=1)\n",
    "ax6.set_title('Algorithm Agreement Matrix', fontsize=12, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print summary\n",
    "print(\"\\nüèÜ BEST ALGORITHM:\")\n",
    "best_idx = np.argmax(accuracies)\n",
    "print(f\"   {algorithms[best_idx]} with {accuracies[best_idx]:.3f} accuracy\")\n",
    "\n",
    "print(\"\\nüìä ALGORITHM STRENGTHS:\")\n",
    "print(f\"   Decision Tree: Interpretability ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | Speed ‚≠ê‚≠ê‚≠ê‚≠ê\")\n",
    "print(f\"   KNN: Simplicity ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | Non-parametric ‚≠ê‚≠ê‚≠ê‚≠ê\")\n",
    "print(f\"   SVM: Margin optimization ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | Kernel flexibility ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64d61cec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom test satellite input\n",
    "test_sat1 = {\n",
    "    'distance': 8.5,\n",
    "    'relative_velocity': 2.3,\n",
    "    'sat1_altitude': 600,\n",
    "    'sat2_altitude': 650,\n",
    "    'altitude_diff': 50\n",
    "}\n",
    "\n",
    "test_sat2 = {\n",
    "    'distance': 3.2,\n",
    "    'relative_velocity': 4.1,\n",
    "    'sat1_altitude': 1200,\n",
    "    'sat2_altitude': 1180,\n",
    "    'altitude_diff': 20\n",
    "}\n",
    "\n",
    "test_cases = pd.DataFrame([test_sat1, test_sat2])\n",
    "X_custom = test_cases[feature_cols].values\n",
    "\n",
    "# Scale for KNN and SVM\n",
    "X_custom_scaled = scaler_knn.transform(X_custom)\n",
    "\n",
    "# Predictions from all algorithms\n",
    "dt_pred = dt_classifier.predict(X_custom)\n",
    "knn_pred = knn_classifier.predict(X_custom_scaled)\n",
    "svm_pred = svm_classifier.predict(X_custom_scaled)\n",
    "\n",
    "risk_labels = ['SAFE', 'CAUTION', 'HIGH_RISK']\n",
    "\n",
    "print(\"Custom Satellite Collision Predictions:\\n\")\n",
    "print(\"Test Case 1:\")\n",
    "print(f\"  Distance: {test_sat1['distance']}km, Velocity: {test_sat1['relative_velocity']}km/s\")\n",
    "print(f\"  Decision Tree: {risk_labels[dt_pred[0]]}\")\n",
    "print(f\"  KNN: {risk_labels[knn_pred[0]]}\")\n",
    "print(f\"  SVM: {risk_labels[svm_pred[0]]}\")\n",
    "\n",
    "print(\"\\nTest Case 2:\")\n",
    "print(f\"  Distance: {test_sat2['distance']}km, Velocity: {test_sat2['relative_velocity']}km/s\")\n",
    "print(f\"  Decision Tree: {risk_labels[dt_pred[1]]}\")\n",
    "print(f\"  KNN: {risk_labels[knn_pred[1]]}\")\n",
    "print(f\"  SVM: {risk_labels[svm_pred[1]]}\")\n",
    "\n",
    "# Visualization\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "for idx, (ax, case_name) in enumerate(zip(axes, ['Test Case 1', 'Test Case 2'])):\n",
    "    predictions = [risk_labels[dt_pred[idx]], risk_labels[knn_pred[idx]], risk_labels[svm_pred[idx]]]\n",
    "    colors = {'SAFE': 'green', 'CAUTION': 'orange', 'HIGH_RISK': 'red'}\n",
    "    bar_colors = [colors[p] for p in predictions]\n",
    "    \n",
    "    ax.bar(['Decision Tree', 'KNN', 'SVM'], [1, 1, 1], color=bar_colors, alpha=0.7, edgecolor='black')\n",
    "    ax.set_ylabel('Prediction')\n",
    "    ax.set_title(f'{case_name} Predictions')\n",
    "    ax.set_ylim([0, 1.5])\n",
    "    ax.set_yticks([])\n",
    "    \n",
    "    for i, pred in enumerate(predictions):\n",
    "        ax.text(i, 0.5, pred, ha='center', va='center', fontweight='bold', fontsize=12)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nTo test your own scenarios, modify the test_sat1 and test_sat2 dictionaries above and re-run this cell.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d212e5e",
   "metadata": {},
   "source": [
    "## üéÆ Section 10: Interactive Playground"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cef91d9a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
